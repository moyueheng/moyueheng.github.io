---
title: 'cuda编程基础'
date: '2024-11-02'
tags: []
draft: false
summary: 
---


## CUDA编程基础

CUDA（Compute Unified Device Architecture）是由NVIDIA开发的一种并行计算平台和编程模型，旨在利用GPU的强大计算能力。通过CUDA，开发者能够在GPU上执行计算密集型的任务，从而显著提高程序的性能。以下是CUDA编程的基本概念和步骤。

****CUDA编程模型概述****

在CUDA编程中，CPU被称为*host*，而GPU被称为*device*。这两者有各自独立的内存空间。CUDA程序通常包括以下几个步骤：

- **内存分配**：在host和device上分别分配内存。
  
- **数据初始化**：在host上初始化数据。

- **数据传输**：将数据从host传输到device。

- **核函数执行**：在device上执行核函数（kernel）。

- **结果传回**：将计算结果从device传回host。

- **内存释放**：释放在host和device上分配的内存。

****CUDA核函数****

核函数是指在GPU上并行执行的函数。使用`__global__`关键字来定义核函数，并通过`<<<grid, block>>>`语法来指定执行配置，其中`grid`表示网格的数量，`block`表示每个网格中的线程块数量。例如：

```cpp
__global__ void vectorAdd(float *A, float *B, float *C, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        C[i] = A[i] + B[i];
    }
}
```

在这个例子中，`vectorAdd`是一个核函数，它将两个向量相加并将结果存储到第三个向量中。

****线程和块的概念****

CUDA中的线程是执行计算的基本单位。每个线程都有一个唯一的ID，可以通过内置变量如`threadIdx`、`blockIdx`和`blockDim`来访问。线程被组织成块（block），多个块组成一个网格（grid）。这种层次结构使得CUDA能够高效地管理大量并行任务。

****内存管理****

CUDA提供了多种内存类型，包括：

- **全局内存（Global Memory）**：所有线程都可以访问，但访问速度较慢。
  
- **共享内存（Shared Memory）**：同一块中的线程可以共享，速度较快，但容量有限。
  
- **局部内存（Local Memory）**：每个线程私有，通常用于存储局部变量。

使用以下API进行内存管理：

- `cudaMalloc()`：在device上分配内存。
  
- `cudaMemcpy()`：在host和device之间传输数据。
  
- `cudaFree()`：释放已分配的设备内存。

例如：

```cpp
float *d_A, *d_B, *d_C;
cudaMalloc(&d_A, N * sizeof(float));
cudaMalloc(&d_B, N * sizeof(float));
cudaMalloc(&d_C, N * sizeof(float));
```

****简单示例：向量加法****

以下是一个完整的向量加法示例，包括主机代码和设备代码：

```cpp
#include <stdio.h>

__global__ void vectorAdd(float *A, float *B, float *C, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        C[i] = A[i] + B[i];
    }
}

int main() {
    int N = 1 << 20; // 1M elements
    size_t size = N * sizeof(float);

    // 在主机上分配内存
    float *h_A = (float *)malloc(size);
    float *h_B = (float *)malloc(size);
    float *h_C = (float *)malloc(size);

    // 初始化向量
    for (int i = 0; i < N; i++) {
        h_A[i] = i;
        h_B[i] = i;
    }

    if (h_A == NULL || h_B == NULL || h_C == NULL) {
        printf("Memory allocation failed\n");
        return 1;
    }

    functio
    // 在设备上分配内存
    float *d_A, *d_B, *d_C;
    cudaMalloc(&d_A, size);
    cudaMalloc(&d_B, size);
    cudaMalloc(&d_C, size);

    // 将数据从主机复制到设备
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // 启动核函数
    int threadsPerBlock = 256;
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

    // 将结果从设备复制回主机
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // 验证结果
    for (int i = 0; i < N; i++) {
        if (h_C[i] != h_A[i] + h_B[i]) {
            printf("Error: %f != %f\n", h_C[i], h_A[i] + h_B[i]);
            break;
        }
    }

    // 释放设备和主机内存
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
    free(h_A);
    free(h_B);
    free(h_C);

    return 0;
}
```

这个示例展示了如何使用CUDA进行简单的向量加法，包括内存管理、核函数定义及调用等基本操作。

通过以上内容，你可以对CUDA编程有一个初步的了解，并能够编写简单的CUDA程序。随着对CUDA更深入的学习，你将能够掌握更多优化技巧和高级特性，以充分利用GPU的计算能力。

Citations:
[1] https://www.youtube.com/watch?v=IuxJO0HOcH0
[2] https://blog.csdn.net/weixin_44966641/article/details/124448102
[3] https://godweiyang.com/2021/01/25/cuda-reading/
[4] https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/
[5] https://blog.csdn.net/m0_37870649/article/details/132122059
[6] https://hpcwiki.io/gpu/cuda/
[7] https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/
[8] https://developer.nvidia.com/zh-cn/blog/cuda-model-intro-cn/
[9] https://cuda-tutorial.github.io/part1_22.pdf